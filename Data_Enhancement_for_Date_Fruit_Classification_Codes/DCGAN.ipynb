{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NewGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXEScjhKvR27"
      },
      "source": [
        "# PreProcessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1uzgVILwIrf"
      },
      "source": [
        "**The following packages will be used to implement a basic GAN system in Python/Keras. Importing libraries and layers from tensorflow.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf58yyr7vekn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPEnlYN5mn71"
      },
      "source": [
        "**The following code mounts Google drive for use with Google CoLab**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qd1Wm40CbTU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFPMfpc7w5A_"
      },
      "source": [
        "**Setting hyperparameters for computation.Here, we mention parameters for image resolution, preview image size details,seed size,epochs,batch size and buffer size.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o7c1lwdvxiA"
      },
      "source": [
        "#resolution of images\n",
        "GENERATE_SQUARE = 96 #96x96\n",
        "IMAGE_CHANNELS = 3  #RGB\n",
        "\n",
        "# Preview images for viewing samples during training\n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "# Size vector to generate images from (noise vector)\n",
        "SEED_SIZE = 100  \n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = '/content/dates/sukkri' #copy file location of unzipped data\n",
        "EPOCHS = 2000\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 600\n",
        "\n",
        "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k91Wf_obvYTi"
      },
      "source": [
        "**We store the processed file as a binary. This way we can simply reload the processed training data and quickly use it. It is most efficient to only perform this operation once. The dimensions of the image are encoded into the filename of the binary file because we need to regenerate it if these change. Converting input images from the dataset to arrays. As the colored images have values of varying from (0,255) pixel values, scaling it to (-1, 1) as of generator's last layer's activation Tanh range.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGj4-cXsv2Tm"
      },
      "source": [
        "#converting of (255,255) pixel value to (-1, 1) as of generator's last layer's activation Tanh range\n",
        "\n",
        "training_binary_path = os.path.join(DATA_PATH,\n",
        "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
        "\n",
        "\n",
        "print(f\"Looking for file: {training_binary_path}\")\n",
        "\n",
        "if not os.path.isfile(training_binary_path):\n",
        "  print(\"Loading training images...\")\n",
        "\n",
        "  training_data = []\n",
        "  faces_path = DATA_PATH\n",
        "  for filename in tqdm(os.listdir(faces_path)):\n",
        "      path = os.path.join(faces_path,filename)\n",
        "      image = Image.open(path).resize((GENERATE_SQUARE,GENERATE_SQUARE),Image.ANTIALIAS)\n",
        "      training_data.append(np.asarray(image))\n",
        "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS))\n",
        "  training_data = training_data.astype(np.float32)\n",
        "  training_data = training_data / 127.5 - 1.\n",
        "\n",
        "\n",
        "  print(\"Saving training image binary...\")\n",
        "  np.save(training_binary_path,training_data)\n",
        "else:\n",
        "  print(\"Loading previous training pickle...\")\n",
        "  training_data = np.load(training_binary_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDgenb7MvdlB"
      },
      "source": [
        "**We will use a TensorFlow Dataset object to actually hold the images. This allows the data to be quickly shuffled int divided into the appropriate batch sizes for training.Making batches of dataset for training the model in batches.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlH6DaqWv-7r"
      },
      "source": [
        "#making batches of data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ZsFJzzwFNz"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0MLB9GI2Vsi"
      },
      "source": [
        "**Making a generator network using Sequential model to add the following layers.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZQLbagtwCPH"
      },
      "source": [
        "#generator network\n",
        "generator = keras.models.Sequential([\n",
        "    keras.layers.Dense(6*6*256, input_shape=[SEED_SIZE]),\n",
        "    keras.layers.Reshape([6, 6, 256]),\n",
        "    keras.layers.Conv2DTranspose(64, (5,5), (2,2), padding=\"same\"),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(32, (5,5), (2,2), padding=\"same\"),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(32, (5,5), (2,2), padding=\"same\"),\n",
        "    keras.layers.ReLU(),\n",
        "    keras.layers.Conv2DTranspose(3, (5,5), (2,2), padding=\"same\", activation=\"tanh\"),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QURmMl5yP9O"
      },
      "source": [
        "**Input, output dimensions and parameters of every layer of generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkC2p9MEXNu4"
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riyatdem2s1e"
      },
      "source": [
        "**Making a discriminator network using Sequential model to add the following layers.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5WjRovywMva"
      },
      "source": [
        "#discriminator network\n",
        "discriminator = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, (5,5), (2,2), padding=\"same\", input_shape=[96, 96, 3]),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Conv2D(64, (5,5), (2,2), padding=\"same\"),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(64, (5,5), (2,2), padding=\"same\"),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(128, (5,5), (2,2), padding=\"same\"),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1HYltmvynVi"
      },
      "source": [
        "**Input,output dimensions and parameters of every layer of discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU7kcmANwTTM"
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZqo8DcwWVJ"
      },
      "source": [
        "# Saving images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5q__tZ_29fH"
      },
      "source": [
        "**OUT_PATH gives the destination to store the images that our deep-convolutional gan network will generate.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUWSNGBBD5Pw"
      },
      "source": [
        "mkdir '/content/drive/newImages/final_sukkri'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qwqT3lD_A2_"
      },
      "source": [
        "OUT_PATH='/content/drive/newImages/final_sukkri'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o23aoA93nkn"
      },
      "source": [
        "**As we progress through training images will be produced to show the progress.These images will contain a number of rendered dates that show how good the generator has become.Below cell has a function to save coloured images generated by dcgan in 7x4 grid to preview the images.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIa-hk2xwYNf"
      },
      "source": [
        "def save_images(cnt,noise):\n",
        "  image_array = np.full(( \n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
        "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
        "      255, dtype=np.uint8)\n",
        "  \n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  generated_images = 0.5 * generated_images + 0.5\n",
        "\n",
        "  image_count = 0\n",
        "  for row in range(PREVIEW_ROWS):\n",
        "      for col in range(PREVIEW_COLS):\n",
        "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] = generated_images[image_count] * 255\n",
        "        image_count += 1\n",
        "\n",
        "          \n",
        "  output_path = os.path.join(OUT_PATH,f\"output-{cnt}\")\n",
        "  if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "  \n",
        "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
        "  im = Image.fromarray(image_array)\n",
        "  im.save(filename)\n",
        "  generator.save(os.path.join(output_path,f\"face_generator-{cnt}.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8hVXBrKwaAE"
      },
      "source": [
        "# Testing networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOrLDh2N44B4"
      },
      "source": [
        "**Before training the model, generate random sample by adding noise in generator network and specifying training as false.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K12WtZvwdQ6"
      },
      "source": [
        "#generating random sample before training\n",
        "noise = tf.random.normal([1, SEED_SIZE])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlM1Qmqg5ONX"
      },
      "source": [
        "**Checking discriminator's decision on the random sample generated.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftgn9MTtwfAc"
      },
      "source": [
        "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-fE0lUxwnzr"
      },
      "source": [
        "# Compiling Gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6wCaCTW5oHp"
      },
      "source": [
        "**For the generator training set, the $x$ contains the random seeds to generate images and the $y$ always contains the value of 1, because the optimal is for the generator to have generated such good images that the discriminiator was fooled into assigning them a probability near 1.Defining loss for generator and discriminator networks. Binary crossentropy loss has been used. For the generator network the loss is only determined by the fake output that it generates.For discriminator network,total loss is the sum of loss for real and fake outputs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9lBubrA-mbt"
      },
      "source": [
        "#defining losses\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYWb8pE_6VZE"
      },
      "source": [
        "**In this cell optimizer for the generator and discriminator networks have been defined. Adam optimizer(with a learning rate of 1.5e-4) is used for both the networks.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LruvslEq-rtz"
      },
      "source": [
        "#defining otptimizer\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENsy6NUg-uBM"
      },
      "source": [
        "# Training the Gan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6EwKX8Y9T_5"
      },
      "source": [
        "**Defining a fuction for train the dcgan. We train the model by giving randomly generated seed values to the generator. The generator gives an output image, this image along with real image is feeded to the discriminator network. Gradients are calculated for both these network in the basis of generator loss and discriminator loss. Later, optimization is done using these gradients.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZBGWwud-wL0"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(seed, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)#calculating gradients of generator\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)#calculating gradients of discriminator\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))#applying gradients with the optimizer\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))#applying gradients with the optimizer\n",
        "  return gen_loss,disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIzn7Gjn-xfx"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "\n",
        "    gen_loss_list = []\n",
        "    disc_loss_list = []\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      t = train_step(image_batch)\n",
        "      gen_loss_list.append(t[0])\n",
        "      disc_loss_list.append(t[1])\n",
        "\n",
        "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
        "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
        "\n",
        "\n",
        "    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}')\n",
        "    if epoch%100==0:\n",
        "      save_images(epoch,fixed_seed)\n",
        "\n",
        "\n",
        "  print (f'Training time: {hms_string(epoch_elapsed)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8z7xr0u_3iO"
      },
      "source": [
        "**Calling the train fuction to start train the gan model for desired number of epochs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxTf-xVv-9-n"
      },
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biO-kawQL1SM"
      },
      "source": [
        "# Generating images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-23wK378BCI7"
      },
      "source": [
        "**In this section 500 generated images are saved.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7lWylzxNina"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgCSgnWVP3uT"
      },
      "source": [
        "**Load the best model to generate images of dates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn28QXDhHEL_"
      },
      "source": [
        "#loading best model\n",
        "generator = keras.models.load_model('/content/drive/My Drive/minor/final_aloa/output-200/face_generator-200.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boH-RvYeP9h3"
      },
      "source": [
        "**Check generator's working by plotting a generated image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve5TDxkaJKJx"
      },
      "source": [
        "noise = tf.random.normal([1, SEED_SIZE])\n",
        "generated_image = generator(noise, training=False)\n",
        "generated_image = 0.5 * generated_image + 0.5\n",
        "plt.imshow(generated_image[0, :, :, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIf6KZc5Oiq5"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "337ql8SsQGl5"
      },
      "source": [
        "**Define the directory where results folder will be made**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVDFw923OCog"
      },
      "source": [
        "OUT_PATH_samples='/content/drive/My Drive/minor/final_aloa'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IXnsaAMOHub"
      },
      "source": [
        "output_samples_path = os.path.join(OUT_PATH_samples,f\"results\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzKFvnQ6QUfM"
      },
      "source": [
        "**Make results folder in OUT_PATH_SAMPLES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOHJxCCeOLGD"
      },
      "source": [
        "!mkdir '/content/drive/My Drive/minor/final_aloa/results'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBcklNYyQdrs"
      },
      "source": [
        "**Below cell saves 500 synthetically generated images to the output_samples_path.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlmIVIp1OOp-"
      },
      "source": [
        "x=500 # no of samples to be generated\n",
        "for i in range(x):\n",
        "  noise = tf.random.normal([1, SEED_SIZE])\n",
        "  generated_image = generator(noise, training=False)\n",
        "  generated_image = 0.5 * generated_image + 0.5\n",
        "  generated_image=generated_image[0, :, :, :]\n",
        "  filename = os.path.join(output_samples_path,f\"aloa-{i}.png\")\n",
        "  generated_image=np.asarray(generated_image)\n",
        "  im = Image.fromarray((generated_image * 255).astype(np.uint8))\n",
        "  im.save(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}